{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv2D-jL5B-CZ"
      },
      "source": [
        "# LangChain Expression Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzaY-p_-B-Ca"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wesfSdRJB-Ca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-I9SzTMB-Ca"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnablePassthrough,\n",
        "    RunnableParallel,\n",
        "    RunnableLambda,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoORFwpoB-Ca"
      },
      "source": [
        "---\n",
        "\n",
        "## Accessing Previous Values using RunnablePassThrough\n",
        "\n",
        "A runnable to passthrough inputs unchanged or with additional keys.\n",
        "\n",
        "This runnable behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dict.\n",
        "\n",
        "The examples below demonstrate this runnable works using a few simple chains. The chains rely on simple lambdas to make the examples easy to execute and experiment with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QecrSYQB-Cb",
        "outputId": "fd634da8-6306-421f-9d3f-697a3955870a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'origin': 1, 'modified': 2}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'original': 'hello world', 'parsed': 'dlrow olleh'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "runnable = RunnableParallel(\n",
        "    origin=RunnablePassthrough(),\n",
        "    modified=lambda x: x+1\n",
        ")\n",
        "\n",
        "print(runnable.invoke(1)) # {'origin': 1, 'modified': 2}\n",
        "\n",
        "\n",
        "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
        "    return prompt + \" world\"\n",
        "\n",
        "chain = RunnableLambda(fake_llm) | {\n",
        "    'original': RunnablePassthrough(), # Original LLM output\n",
        "    'parsed': lambda text: text[::-1] # Parsing logic\n",
        "}\n",
        "\n",
        "chain.invoke('hello')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO3mlD9-B-Cb"
      },
      "source": [
        "---\n",
        "\n",
        "## Prompt + Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_g7S7t_B-Cb",
        "outputId": "24a3e1e0-5783-477e-89fc-83fa895cafb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first=ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a joke about {topic}'))]) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f9b10a1a550>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f9b0932c220>, openai_api_key='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', openai_proxy='')\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate.from_template('Tell me a joke about {topic}')\n",
        "\n",
        "chain = prompt | chat\n",
        "print(chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1TNxG61B-Cb",
        "outputId": "88df737b-3d00-47ac-d280-259bbf2e5ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first input_variables=['topic'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a joke about {topic}'))]\n",
            "last client=<openai.resources.chat.completions.Completions object at 0x7f9b10a1a550> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f9b0932c220> openai_api_key='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' openai_proxy=''\n"
          ]
        }
      ],
      "source": [
        "print(\"first\", chain.first)\n",
        "print(\"last\", chain.last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekXdXpYzB-Cb",
        "outputId": "1763c0e3-b1b1-4092-8589-d62c1595f69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Stream:\n",
            "\n",
            "Why don't bears wear shoes? \n",
            "\n",
            "Because they have bear feet!\n",
            "\n",
            "Invoke:\n",
            "\n",
            "Why don't bears wear shoes?\n",
            "\n",
            "Because they already have bear feet!\n",
            "\n",
            "\n",
            "Batch:\n",
            "\n",
            "[AIMessage(content=\"Why don't bears ever wear shoes?\\n\\nBecause they already have bear feet!\"), AIMessage(content=\"Why don't bears use cell phones? \\nBecause they can't bear to have their calls dropped!\"), AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")]\n"
          ]
        }
      ],
      "source": [
        "# Stream:\n",
        "print('\\n\\nStream:\\n')\n",
        "for s in chain.stream({\"topic\": \"bears\"}):\n",
        "    print(s.content, end=\"\", flush=True)\n",
        "\n",
        "# Invoke:\n",
        "print('\\n\\nInvoke:\\n')\n",
        "print(chain.invoke({\"topic\": \"bears\"}).content)\n",
        "\n",
        "# Batch:\n",
        "print('\\n\\nBatch:\\n')\n",
        "print(chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"bears\"}, {\"topic\": \"bears\"}]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ToxbtXzB-Cb"
      },
      "source": [
        "---\n",
        "\n",
        "## Retrieval Augmented Generation (RAG) in LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LpWNLREB-Cb",
        "outputId": "32d44ad8-ead8-4d07-f110-ba6518016559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain openai faiss-cpu tiktoken langchain-community --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vV30YYP6B-Cb"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.vectorstores.faiss import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBM35GwqB-Cc"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_texts(\n",
        "    [\"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\", \"James has an age of 31 years old.\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Uve1ivB-Cc"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# It's the same as this, but the tuple allows for line breaks:\n",
        "# {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUCJtbvdB-Cc",
        "outputId": "dadf82b6-fc5c-4625-a8e7-4a4c19b51361"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'James Phoenix works at JustUnderstandingData.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"What company does James phoenix work at?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyX2W-2B-Cc",
        "outputId": "74405d17-b30c-499f-bd76-8529c66654f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"James Phoenix's age is 31 years old.\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"What is James Phoenix's age?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W4Bha9XB-Cc"
      },
      "source": [
        "---\n",
        "\n",
        "## Understanding How `itemgetter` Works with Piping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FeqI7E0B-Cc",
        "outputId": "a889c97c-9616-41ab-b60d-299c7c2e370e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "operator.itemgetter({'data': ['This is a test', 'Another entry...']})\n",
            "['This is a test', 'Another entry...']\n"
          ]
        }
      ],
      "source": [
        "test = {\n",
        "    \"data\": ['This is a test', 'Another entry...']\n",
        "}\n",
        "\n",
        "print(itemgetter(test))\n",
        "print(itemgetter('data')(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH22HLELB-Cc"
      },
      "source": [
        "### How does it work within the context of LCEL?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3LYvdePB-Cc",
        "outputId": "d4090841-48b4-4f12-9c5e-c4222d661fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'James Phoenix is a Data Engineer.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template('''What is the profession of James Phoenix? His profession is {profession}.''')\n",
        "\n",
        "first_chain = RunnableParallel(\n",
        "    name=lambda x: \"James Phoenix\",\n",
        "    age=lambda x: 31\n",
        ")\n",
        "\n",
        "second_chain = {\n",
        "    # itemgetter is used to get the value from the dictionary from the previous step: (note this is only the previous step, not the whole chain)\n",
        "    'name': itemgetter('name'),\n",
        "    'age': itemgetter('age'),\n",
        "    # You can not use string values, either use itemgetter or a lambda, or RunnablePassthrough\n",
        "    'profession': lambda x: \"Data Engineer\"\n",
        "}\n",
        "\n",
        "chain = first_chain | second_chain |  prompt |  ChatOpenAI() | StrOutputParser()\n",
        "chain.invoke({})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}