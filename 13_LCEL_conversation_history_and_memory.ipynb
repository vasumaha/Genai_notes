{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasumaha/Genai_notes/blob/main/13_LCEL_conversation_history_and_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSPeQyoHB5Cm"
      },
      "source": [
        "# LangChain Expression Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umZSjMxlB5Cn"
      },
      "outputs": [],
      "source": [
        "%pip install langchain faiss-cpu langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktviBByB5Co"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MndBYaNvB5Co"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.prompts import format_document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S1_ask1B5Cp"
      },
      "source": [
        "---\n",
        "\n",
        "## Adding In Conversational History:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XLoxQbmB5Cp"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_texts(\n",
        "    [\n",
        "        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n",
        "        \"James is 31 years old.\",\n",
        "    ],\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKp6kC03B5Cp"
      },
      "outputs": [],
      "source": [
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOZaxbPjB5Cp"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXOewC8-B5Cp"
      },
      "outputs": [],
      "source": [
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqHYfopuB5Cp"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "def _format_chat_history(chat_history: List[Union[HumanMessage, SystemMessage, AIMessage]]) -> str:\n",
        "    buffer = \"\"\n",
        "    for dialogue_turn in chat_history:\n",
        "        if isinstance(dialogue_turn, HumanMessage):\n",
        "            buffer += \"\\nHuman: \" + dialogue_turn.content\n",
        "        elif isinstance(dialogue_turn, AIMessage):\n",
        "            buffer += \"\\nAssistant: \" + dialogue_turn.content\n",
        "        elif isinstance(dialogue_turn, SystemMessage):\n",
        "            buffer += \"\\nSystem: \" + dialogue_turn.content\n",
        "    return buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5gMEhzJB5Cq"
      },
      "outputs": [],
      "source": [
        "_inputs = RunnableMap(\n",
        "    standalone_question=RunnablePassthrough.assign(\n",
        "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "    )\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser(),\n",
        ")\n",
        "_context = {\n",
        "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-jvemZsB5Cq",
        "outputId": "df5968fc-6499-433f-fb30-30b1e288a666"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='James Phoenix was employed at JustUnderstandingData.')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_qa_chain.invoke(\n",
        "    {\n",
        "        \"question\": \"where did James work?\",\n",
        "        \"chat_history\": [],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALr_zWBnB5Cq"
      },
      "source": [
        "---\n",
        "\n",
        "## Adding Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoGFgbTMB5Cq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOjgaIpKB5Cr"
      },
      "outputs": [],
      "source": [
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
        "    }\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser(),\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "\n",
        "# This is REALLY IMPORTANT as the chain above becomes StrOutputParser() so it will only have one key, which gets passed to the retriever!\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "}\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbP7xqB-B5Cr",
        "outputId": "bbcd1949-8409-459f-af12-25b3017d47b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': AIMessage(content='James Phoenix was employed at JustUnderstandingData.'), 'docs': [Document(page_content='James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData'), Document(page_content='James is 31 years old.')]}\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"question\": \"where did James Phoenix work?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2baZrAzB5Cr"
      },
      "outputs": [],
      "source": [
        "# Note that the memory does not save automatically\n",
        "# This will be improved in the future\n",
        "# For now you need to save it yourself\n",
        "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbuUh3TPB5Cr",
        "outputId": "9f0ebf95-5d96-414c-c6a4-14727f98c3fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='where did James Phoenix work?'),\n",
              "  AIMessage(content='James Phoenix was employed at JustUnderstandingData.')]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVRMyA4LB5Cr"
      },
      "source": [
        "------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}